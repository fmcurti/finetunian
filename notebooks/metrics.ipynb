{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset,DatasetDict,Dataset,Audio\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    torch_dtype = torch.float32\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced71fb4269745fcbe18fe2900d04d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('audiofolder', data_dir=r'C:\\Users\\Felip\\OneDrive\\Desktop\\Dev\\finetunian\\dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "asr = pipeline(\n",
    "  \"automatic-speech-recognition\",\n",
    "  model=\"./whisper-small-dv/checkpoint-100\",\n",
    "  return_language=\"english\",\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutomaticSpeechRecognitionPipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mAutomaticSpeechRecognitionPipeline\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutomaticSpeechRecognitionPipeline' is not defined"
     ]
    }
   ],
   "source": [
    "AutomaticSpeechRecognitionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felip\\OneDrive\\Desktop\\Dev\\finetunian\\venv\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' Grīgs esmu viņu izveikā, ka esi esmu izveikātas ir tevērītas, ka ir tevērītas.',\n",
       " 'chunks': [{'language': 'latvian',\n",
       "   'text': ' Grīgs esmu viņu izveikā, ka esi esmu izveikātas ir tevērītas, ka ir tevērītas.'}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr(train_data[5]['audio'], return_language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felip\\OneDrive\\Desktop\\Dev\\finetunian\\venv\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:312: FutureWarning: `max_new_tokens` is deprecated and will be removed in version 4.49 of Transformers. To remove this warning, pass `max_new_tokens` as a key inside `generate_kwargs` instead.\n",
      "  warnings.warn(\n",
      "100%|██████████| 79/79 [01:06<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "\n",
    "# run streamed inference\n",
    "for prediction in tqdm(\n",
    "    asr(\n",
    "        KeyDataset(train_data, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\"},\n",
    "        batch_size=2,\n",
    "    ),\n",
    "    total=len(train_data),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Please call Stella. Ask her to bring these things with her from the store. 6 spoons of fresh snow peas, 5 thick slabs of blue cheese and maybe a snack for her brother Bob. We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into 3 red bags. And we will go meet her Wednesday at the train station.',\n",
       " ' The revised procedure was acclaimed as a long-overdue reform.',\n",
       " ' The revised procedure was acclaimed as a long-overdue reform.',\n",
       " ' Vēl kordija esmu magnificinļu dekorājot.',\n",
       " ' Vēl kotiāt esmu magnificindu degerēt.',\n",
       " ' Grīgs esmu viņu izveikā, ka esi esmu izveikātas ir tevērītas, ka ir tevērītas.',\n",
       " ' Vienosmeni vispārīdāt, ko viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir viņa ir vi',\n",
       " ' Nauhseman katsotaan rainvojen asioista, jossa katsotaan katsomia jatkuuun.',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' Others have tried to explain the phenomenon physically.',\n",
       " ' Ar iztodu, tad, bet arī raimbu varu kozītas refliktas ar uzsensu, arī raimu.',\n",
       " ' Esmu vienkārši fizicisti, ko ir nekārši reflikti, bet reflikti, ka ir raindlopas, ko ir raimbos.',\n",
       " ' Siksi sitten fysiskiville löysi, että se ei ole reflektiosta, mutta refraktionen rannan rauhdon, joka kautta rauhdon.',\n",
       " ' he smashed it in and tumbled into darkness',\n",
       " ' Han smastet inn og tummelt inn til dårlig næse.',\n",
       " \" it was like finally getting into one's own nightmares to punish one's dreams.\",\n",
       " \" It was like finally getting into one's own nightmares to punish one's dreams.\",\n",
       " ' She found herself able to sing any role and any song which struck her fancy.',\n",
       " ' Kādsība, ko vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl vēl v',\n",
       " ' Lips prasms morni fulvi. Vizsterdājumā ar iztrājus visāk un leftsaidu.',\n",
       " 'Lips pursed mournfully, he stared down at its crazily sagging left side.',\n",
       " ' Kārša arī ir vienkārši arī, ka arī ir vienkārši arī ir prismas.',\n",
       " ' मेज्ट्रप',\n",
       " ' बाज़ाए बाज़ा',\n",
       " ' Mēķūrāp.',\n",
       " ' made urge',\n",
       " ' made urge',\n",
       " ' made vest',\n",
       " ' made vest',\n",
       " ' made vest',\n",
       " ' Mēģu vajā.',\n",
       " ' Mēdu vajā.',\n",
       " ' Mēdvēl.',\n",
       " ' Nirdžuri.',\n",
       " ' neared jury',\n",
       " ' Pit red',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' Kārša arī ir vienkārši arī, ka arī ir vienkārši arī ir prismas.',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' Pleks kap.',\n",
       " 'plex drum',\n",
       " 'plex drum',\n",
       " ' ಸಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' Plexiç',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' ಸಾರಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿ',\n",
       " ' Pleks, fine?',\n",
       " ' Plex gust.',\n",
       " ' Pleks kast.',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' Plex harp.',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' ಠಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿ',\n",
       " 'plex jury',\n",
       " ' Plex jury.',\n",
       " 'plex jury',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ',\n",
       " ' प्यक्स्ड़््ध',\n",
       " ' These take the shape of a long round arc, with its path high above, and its two ends apparently beyond the horizon.',\n",
       " ' Mēs esam kuru futšu, ko esi vajadzījās izgrļēt.',\n",
       " ' Betoskotu fadžu gauz vēl, vispār, ka neviņa izkrim.',\n",
       " ' Granā arī vēcēbā garļu.',\n",
       " ' Greenbees ja vetsävalta.',\n",
       " ' Es nekārši, bet lēdzi, ka neko košu kārši kārši.',\n",
       " ' People look, but no one ever finds it.',\n",
       " ' When a man looks for something beyond his reach, his friends say he is looking for the pot of gold at the end of the rainbow.',\n",
       " 'Nuclear rockets can destroy airfields with ease.',\n",
       " ' Turbāk uzsājumus, kāds ir ļoti arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī arī',\n",
       " ' Esam uzsusījām ir amerikākā, bet visi visi visi eksplonāciju.',\n",
       " ' Paldījās, ka brūsītās, ko viņa nebūt univerisākās.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(train_data, all_predictions, label_key='transcription'):\n",
    "    wer_metric = load(\"wer\")\n",
    "\n",
    "    wer_ortho = 100 * wer_metric.compute(\n",
    "        references=train_data[label_key], predictions=all_predictions\n",
    "    )\n",
    "    \n",
    "    normalizer = BasicTextNormalizer()\n",
    "\n",
    "    # compute normalised WER\n",
    "    all_predictions_norm = [normalizer(pred) for pred in all_predictions]\n",
    "    all_references_norm = [normalizer(label) for label in train_data[label_key]]\n",
    "\n",
    "    # filtering step to only evaluate the samples that correspond to non-zero references\n",
    "    all_predictions_norm = [\n",
    "        all_predictions_norm[i]\n",
    "        for i in range(len(all_predictions_norm))\n",
    "        if len(all_references_norm[i]) > 0\n",
    "    ]\n",
    "    all_references_norm = [\n",
    "        all_references_norm[i]\n",
    "        for i in range(len(all_references_norm))\n",
    "        if len(all_references_norm[i]) > 0\n",
    "    ]\n",
    "\n",
    "    wer = 100 * wer_metric.compute(\n",
    "        references=all_references_norm, predictions=all_predictions_norm\n",
    "    )\n",
    "\n",
    "    return wer, (100 - wer), wer_ortho, (100 - wer_ortho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176.54109589041096, -76.54109589041096, 81.97573656845753, 18.024263431542465)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(train_data, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cb3583b8aa4f7db2226258bf30723b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = DatasetDict()\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"en\", split=\"validation\", trust_remote_code=True,streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 16372it [00:00, 25284.40it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "subset_common_voice = {}\n",
    "subset_common_voice['test'] = []\n",
    "for data in common_voice['test']:\n",
    "    if i == 200:\n",
    "        break\n",
    "    subset_common_voice['test'].append(data)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = subset_common_voice['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felip\\OneDrive\\Desktop\\Dev\\finetunian\\venv\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:312: FutureWarning: `max_new_tokens` is deprecated and will be removed in version 4.49 of Transformers. To remove this warning, pass `max_new_tokens` as a key inside `generate_kwargs` instead.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/199 [00:00<?, ?it/s]c:\\Users\\Felip\\OneDrive\\Desktop\\Dev\\finetunian\\venv\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "100%|██████████| 199/199 [01:27<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "\n",
    "# run streamed inference\n",
    "for prediction in tqdm(\n",
    "    asr(\n",
    "        KeyDataset(test_data, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\"},\n",
    "        batch_size=2,\n",
    "    ),\n",
    "    total=len(test_data),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.595416043846539, 84.40458395615346, 20.887991927346114, 79.11200807265388)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(test_data, all_predictions, label_key='sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(test_data, all_predictions, label_key='sentence')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
