{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset,DatasetDict,Dataset,Audio\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    torch_dtype = torch.float32\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199907bf675d4f829ffccc422e7e9164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('audiofolder', data_dir=r'../dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "asr = pipeline(\n",
    "  \"automatic-speech-recognition\",\n",
    "  #model=\"openai/whisper-medium\",\n",
    "  model=\"../models/whisper-base-finetunian/checkpoint-150\",\n",
    "  return_language=\"english\",\n",
    "  device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felip\\OneDrive\\Desktop\\Dev\\finetunian\\venv\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:312: FutureWarning: `max_new_tokens` is deprecated and will be removed in version 4.49 of Transformers. To remove this warning, pass `max_new_tokens` as a key inside `generate_kwargs` instead.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/108 [00:00<?, ?it/s]c:\\Users\\Felip\\OneDrive\\Desktop\\Dev\\finetunian\\venv\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "100%|██████████| 108/108 [00:33<00:00,  3.24it/s]\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "\n",
    "# run streamed inference\n",
    "for prediction in tqdm(\n",
    "    asr(\n",
    "        KeyDataset(train_data, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\", \"language\": \"english\"},\n",
    "        batch_size=2,\n",
    "    ),\n",
    "    total=len(train_data),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please call Stella.Ask her to bring these things with her from the store. Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.We also need a small plastic snake and a big toy frog for the kids.She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.',\n",
       " 'The revised procedure was acclaimed as a long-overdue reform.',\n",
       " 'The revised procedure was acclaimed as a long-overdue reform.',\n",
       " 'The courtyard is magnificently decorated.',\n",
       " 'The courtyard is magnificently decorated.',\n",
       " 'The Greeks used to imagine that it was a sign from the gods to foretell war or heavy rain.',\n",
       " 'Their work mirrors the mentality of the psychopath, rootless and irresponsible.',\n",
       " 'The Norsemen considered the rainbow as a bridge over which the gods passed from earth to their home in the sky.',\n",
       " 'The Norsemen considered the rainbow as a bridge over which the gods passed from earth to their home in the sky.',\n",
       " 'Others have tried to explain the phenomenon physically.',\n",
       " 'Others have tried to explain the phenomenon physically.',\n",
       " \"Aristotle thought that the rainbow was caused by reflection of the sun's rays by the rain.\",\n",
       " 'Since then physicists have found that it is not reflection, but refraction by the raindrops which causes the rainbows.',\n",
       " 'Since then physicists have found that it is not reflection, but refraction by the raindrops which causes the rainbow.',\n",
       " 'He smashed it in and tumbled into darkness.',\n",
       " 'He smashed it in and tumbled into darkness.',\n",
       " \"It was like finally getting into one's own nightmares to punish one's dreams.\",\n",
       " \"It was like finally getting into one's own nightmares to punish one's dreams.\",\n",
       " 'She found herself able to sing any role and any song which struck her fancy.',\n",
       " 'When she awoke, she was the ship.',\n",
       " 'Lips pursed mournfully, he stared down at its crazily sagging left side.',\n",
       " 'Lips pursed mournfully, he stared down at its crazily sagging left side.',\n",
       " 'When the sunlight strikes raindrops in the air, they act as a prism and form a rainbow.',\n",
       " \"Maybe you and me will, girlie, but these two ain't goin' nowhere.\",\n",
       " 'Or the bay of female dogs in heat.',\n",
       " 'made trap',\n",
       " 'made trap',\n",
       " 'made trap',\n",
       " 'made urge',\n",
       " 'made urge',\n",
       " 'made vest',\n",
       " 'made vest',\n",
       " 'made vest',\n",
       " 'made whale',\n",
       " 'made whale',\n",
       " 'made whale',\n",
       " 'near jury',\n",
       " 'near jury',\n",
       " 'odd trap',\n",
       " 'odd trap',\n",
       " 'odd trap',\n",
       " 'odd urge',\n",
       " 'odd urge',\n",
       " 'odd urge',\n",
       " 'odd vest',\n",
       " 'odd vest',\n",
       " 'odd vest',\n",
       " 'pit red',\n",
       " 'pit red',\n",
       " 'When the sunlight strikes raindrops in the air, they act as a prism and form a rainbow.',\n",
       " 'plex cap',\n",
       " 'plex cap',\n",
       " 'plex cap',\n",
       " 'plex drum',\n",
       " 'plex drum',\n",
       " 'plex each',\n",
       " 'plex each',\n",
       " 'plex each',\n",
       " 'plex fine',\n",
       " 'plex fine',\n",
       " 'plex gust',\n",
       " 'plex gust',\n",
       " 'plex harp',\n",
       " 'plex harp',\n",
       " 'plex harp',\n",
       " 'plex sit',\n",
       " 'plex sit',\n",
       " 'plex sit',\n",
       " 'plex jury',\n",
       " 'plex jury',\n",
       " 'plex jury',\n",
       " 'plex crunch',\n",
       " 'plex crunch',\n",
       " 'plex crunch',\n",
       " 'plex crunch',\n",
       " 'plex crunch',\n",
       " 'plex look',\n",
       " 'plex look',\n",
       " 'plex look',\n",
       " 'plex made',\n",
       " 'plex made',\n",
       " 'plex near',\n",
       " 'plex near',\n",
       " 'plex near',\n",
       " 'plex odd',\n",
       " 'plex odd',\n",
       " 'plex pit',\n",
       " 'plex pit',\n",
       " 'plex pit',\n",
       " 'ship crunch',\n",
       " 'ship crunch',\n",
       " 'ship crunch',\n",
       " 'ship look',\n",
       " 'ship look',\n",
       " 'These take the shape of a long round arch, with its path high above, and its two ends apparently beyond the horizon.',\n",
       " 'Butterscotch fudge goes well with vanilla ice cream.',\n",
       " 'Butterscotch fudge goes well with vanilla ice cream.',\n",
       " 'Gwen planted green beans in her vegetable garden.',\n",
       " 'Gwen planted green beans in her vegetable garden.',\n",
       " 'There is, according to legend, a boiling pot of gold at one end.',\n",
       " 'People look, but no one ever finds it.',\n",
       " 'When a man looks for something beyond his reach, his friends say he is looking for the pot of gold at the end of the rainbow.',\n",
       " 'Nuclear rockets can destroy airfields with ease.',\n",
       " 'Throughout the centuries people have explained the rainbow in various ways.',\n",
       " 'Choose carefully between contributory or non-contributory pension plans.',\n",
       " 'Choose carefully between contributory or non-contributory pension plans.',\n",
       " 'Some have accepted it as a miracle without physical explanation.',\n",
       " 'To the Hebrews it was a token that there would be no more universal floods.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(train_data, all_predictions, label_key='transcription'):\n",
    "    wer_metric = load(\"wer\")\n",
    "\n",
    "    wer_ortho = 100 * wer_metric.compute(\n",
    "        references=train_data[label_key], predictions=all_predictions\n",
    "    )\n",
    "    \n",
    "    normalizer = BasicTextNormalizer()\n",
    "\n",
    "    # compute normalised WER\n",
    "    all_predictions_norm = [normalizer(pred) for pred in all_predictions]\n",
    "    all_references_norm = [normalizer(label) for label in train_data[label_key]]\n",
    "\n",
    "    # filtering step to only evaluate the samples that correspond to non-zero references\n",
    "    all_predictions_norm = [\n",
    "        all_predictions_norm[i]\n",
    "        for i in range(len(all_predictions_norm))\n",
    "        if len(all_references_norm[i]) > 0\n",
    "    ]\n",
    "    all_references_norm = [\n",
    "        all_references_norm[i]\n",
    "        for i in range(len(all_references_norm))\n",
    "        if len(all_references_norm[i]) > 0\n",
    "    ]\n",
    "\n",
    "    wer = 100 * wer_metric.compute(\n",
    "        references=all_references_norm, predictions=all_predictions_norm\n",
    "    )\n",
    "\n",
    "    return wer, (100 - wer), wer_ortho, (100 - wer_ortho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.932551319648094, 97.0674486803519, 3.869047619047619, 96.13095238095238)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ORIGINAL\n",
    "get_metrics(train_data, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.4926686217008798, 97.50733137829911, 3.7202380952380953, 96.2797619047619)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finetune2\n",
    "get_metrics(train_data, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16.78082191780822, 83.21917808219177, 24.783362218370883, 75.21663778162912)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Whisper-medium\n",
    "get_metrics(train_data, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.4520547945205475, 95.54794520547945, 6.5857885615251295, 93.41421143847487)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##FINETUNIAN\n",
    "get_metrics(train_data, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cb3583b8aa4f7db2226258bf30723b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = DatasetDict()\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"en\", split=\"validation\", trust_remote_code=True,streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 16372it [00:00, 23376.37it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "subset_common_voice = {}\n",
    "subset_common_voice['test'] = []\n",
    "for data in common_voice['test']:\n",
    "    if i == 350:\n",
    "        break\n",
    "    subset_common_voice['test'].append(data)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = subset_common_voice['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felip\\OneDrive\\Desktop\\Dev\\finetunian\\venv\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:312: FutureWarning: `max_new_tokens` is deprecated and will be removed in version 4.49 of Transformers. To remove this warning, pass `max_new_tokens` as a key inside `generate_kwargs` instead.\n",
      "  warnings.warn(\n",
      "100%|██████████| 349/349 [01:43<00:00,  3.36it/s]\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "\n",
    "# run streamed inference\n",
    "for prediction in tqdm(\n",
    "    asr(\n",
    "        KeyDataset(test_data, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\", \"language\": \"english\"},\n",
    "        batch_size=2,\n",
    "    ),\n",
    "    total=len(test_data),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.061285500747383, 88.93871449925261, 16.296670030272452, 83.70332996972755)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##ORIGINAL\n",
    "get_metrics(test_data, all_predictions, label_key='sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16.243148978574986, 83.756851021425, 22.04843592330979, 77.9515640766902)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##FINETUNIAN\n",
    "get_metrics(test_data, all_predictions, label_key='sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.36075036075036, 89.63924963924964, 15.481786133960046, 84.51821386603996)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##FINETUNIAN2\n",
    "get_metrics(test_data, all_predictions, label_key='sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
